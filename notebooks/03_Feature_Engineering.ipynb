{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfNMFNbgh177E+ouaVnB4d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michele-Maestrini/FusionCore/blob/main/notebooks/03_Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FusionCore v0 â€” Feature Engineering & Temporal Sequencing\n",
        "\n",
        "**Status:** `Architecture Definition`\n",
        "\n",
        "**Author:** Michele Maestrini\n",
        "\n",
        "**Previous Context:** [EDA 02: Regime]\n"
      ],
      "metadata": {
        "id": "L2eEiX285mLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Executive Objective\n",
        "The goal of this notebook is to transform our regime-normalised sensor data into a high-dimensional feature set capable of capturing engine degradation dynamics over time.\n",
        "\n",
        "We will transition from **point-in-time** observations to **sequence-based** inputs, which are essential for Recurrent Neural Networks (RNNs), LSTMs, or Transformers.\n",
        "\n",
        "### Feature Engineering Strategy\n",
        "\n",
        "To build a robust prognostic model, we must move beyond the \"snapshot\" of a single engine cycle. A single sensor reading might tell us the engine is hot, but it doesn't tell us if it is *getting hotter* over time.\n",
        "\n",
        "#### Piecewise Linear RUL (Target Engineering)\n",
        "\n",
        "In real-world aerospace applications, an engine does not begin degrading the moment it leaves the hangar. There is a \"stable\" period followed by a \"wear-out\" period.\n",
        "\n",
        "* **The Logic:** We cap the RUL at a fixed value (e.g., 125 cycles). If the true RUL is 200, we label it as 125. Once the true RUL drops below 125, we follow the linear decline.\n",
        "* **The Benefit:** This prevents the model from trying to learn \"noise\" during the healthy phase and focuses the gradient descent on the actual degradation slope.\n",
        "\n",
        "#### Temporal Windowing (The \"Memory\" Effect)\n",
        "\n",
        "Deep learning models for PHM (Prognostics and Health Management) perform best when fed a 2D \"image\" of recent history.\n",
        "\n",
        "* **Method:** We stack the last $N$ cycles (e.g., $N=30$) into a single input window.\n",
        "* **The Benefit:** This allows the model to detect temporal patterns, such as oscillations or gradual drifts, that are invisible in a single row of data.\n",
        "\n",
        "#### Rolling Statistics (Trend Extraction)\n",
        "\n",
        "While Deep Learning can learn patterns, we can assist the model by explicitly calculating the \"Physics of Change\" through moving averages and standard deviations over the sliding window.\n",
        "\n",
        "* $\\mu_{rolling}$: Captures the current trend (Velocity).\n",
        "* $\\sigma_{rolling}$: Captures increasing instability (Acceleration/Jitter).\n",
        "\n",
        "#### Lagged Differencing ($\\Delta$Health)\n",
        "\n",
        "We will compute the delta between the current cycle and $T-n$ cycles ago.\n",
        "\n",
        "* **The Benefit:** This provides a direct numerical representation of the degradation rate, making it easier for the model to differentiate between a \"Stable High\" reading and a \"Rapidly Rising\" reading."
      ],
      "metadata": {
        "id": "68Fl5Ghs5qFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "1ELO81XN9z0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import pickle  # For saving/loading the engineered features\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9qfYi75T-5L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup"
      ],
      "metadata": {
        "id": "OHWKdYSr92m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set visual style to match our previous notebooks\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n",
        "\n",
        "# British English formatting for plots\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.titleweight'] = 'bold'\n",
        "\n",
        "print(\"--- ðŸ› ï¸ Initialising Feature Engineering Pipeline ---\")"
      ],
      "metadata": {
        "id": "a6QyE4PK96i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DRIVE MOUNTING ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the Project Path\n",
        "DATA_PATH = \"/content/drive/MyDrive/PI/Datasets\"\n",
        "\n",
        "# Check if path exists to prevent errors later\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"âœ… Connection Successful. Data path set to: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"âŒ Error: Path not found at {DATA_PATH}. Please check your directory structure.\")"
      ],
      "metadata": {
        "id": "Ms0UpIev_QaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Data Preprocessing"
      ],
      "metadata": {
        "id": "oNHtHT1rBGpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.1 LEAKAGE-PROOF PREPROCESSING ARCHITECTURE ---\n",
        "\n",
        "def leakage_proof_preprocess(df, settings, sensors, n_clusters=6, train_size=0.8):\n",
        "    \"\"\"\n",
        "    Separates Train/Valid before any statistical parameters are calculated.\n",
        "    LEARNS centroids and stats from Train, then PROJECTS them onto Valid.\n",
        "    \"\"\"\n",
        "    # 1. SPLIT BY UNIT (Ensures an engine is either in Train or Valid, never both)\n",
        "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=42)\n",
        "    train_idx, valid_idx = next(gss.split(df, groups=df['unit_id']))\n",
        "\n",
        "    train_df = df.iloc[train_idx].copy()\n",
        "    valid_df = df.iloc[valid_idx].copy()\n",
        "\n",
        "    # 2. FIT REGIMES (Train Only)\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    train_df['regime'] = kmeans.fit_predict(train_df[settings])\n",
        "\n",
        "    # 3. TRANSFORM REGIMES (Apply Train Centroids to Valid)\n",
        "    # This maps 'unseen' settings to the closest known training cluster\n",
        "    valid_df['regime'] = kmeans.predict(valid_df[settings])\n",
        "\n",
        "    # 4. FIT & TRANSFORM SENSORS (Regime-Specific)\n",
        "    for s in sensors:\n",
        "        if train_df[s].std() > 1e-4:\n",
        "            # Create a lookup table from Train data only\n",
        "            stats = train_df.groupby('regime')[s].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "            # Map Train stats back to Train\n",
        "            train_df = train_df.merge(stats, on='regime', how='left')\n",
        "            train_df[f'{s}_norm'] = (train_df[s] - train_df['mean']) / (train_df['std'] + 1e-9)\n",
        "            train_df.drop(['mean', 'std'], axis=1, inplace=True)\n",
        "\n",
        "            # Map Train stats to Valid (Zero-leakage: Valid uses Train's mu/sigma)\n",
        "            valid_df = valid_df.merge(stats, on='regime', how='left')\n",
        "            valid_df[f'{s}_norm'] = (valid_df[s] - valid_df['mean']) / (valid_df['std'] + 1e-9)\n",
        "            valid_df.drop(['mean', 'std'], axis=1, inplace=True)\n",
        "        else:\n",
        "            train_df[f'{s}_norm'] = 0.0\n",
        "            valid_df[f'{s}_norm'] = 0.0\n",
        "\n",
        "    return train_df, valid_df\n",
        "\n",
        "def execute_full_preprocessing(data_path, train_size=0.8):\n",
        "    \"\"\"\n",
        "    Loops through all FD00x files, applies the leakage-proof logic above.\n",
        "    \"\"\"\n",
        "    index_names = ['unit_id', 'time_cycle']\n",
        "    setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
        "    sensor_names = [f's_{i}' for i in range(1, 22)]\n",
        "    col_names = index_names + setting_names + sensor_names\n",
        "\n",
        "    datasets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
        "    final_splits = {}\n",
        "\n",
        "    for name in datasets:\n",
        "        print(f\"Processing {name}...\")\n",
        "        file_path = os.path.join(data_path, f'train_{name}.txt')\n",
        "\n",
        "        df = pd.read_csv(file_path, sep='\\\\s+', header=None, names=col_names)\n",
        "\n",
        "        # Calculate Base RUL (Global for each unit)\n",
        "        max_cycle = df.groupby('unit_id')['time_cycle'].transform('max')\n",
        "        df['RUL'] = max_cycle - df['time_cycle']\n",
        "\n",
        "        # Call the function defined above\n",
        "        train_df, valid_df = leakage_proof_preprocess(\n",
        "            df,\n",
        "            settings=setting_names,\n",
        "            sensors=sensor_names,\n",
        "            train_size=train_size\n",
        "        )\n",
        "\n",
        "        final_splits[name] = {'train': train_df, 'valid': valid_df}\n",
        "        print(f\"   âœ… {name} Split: {len(train_df['unit_id'].unique())} Train | {len(valid_df['unit_id'].unique())} Valid\")\n",
        "\n",
        "    return final_splits"
      ],
      "metadata": {
        "id": "ObJE9NyhDJ3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXECUTION ---\n",
        "if os.path.exists(DATA_PATH):\n",
        "    split_data = execute_full_preprocessing(DATA_PATH)\n",
        "\n",
        "    # Save immediately to Drive to preserve state\n",
        "    import pickle\n",
        "    with open(os.path.join(DATA_PATH, 'FusionCore_Isolated_Splits.pkl'), 'wb') as f:\n",
        "        pickle.dump(split_data, f)\n",
        "    print(f\"\\nðŸ’¾ Isolated splits saved to {DATA_PATH}\")"
      ],
      "metadata": {
        "id": "EGAPg78QFxGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (2.1) DATASET PREVIEW & SCHEMA AUDIT ---\n",
        "\n",
        "def audit_dataset_splits(split_dict):\n",
        "    \"\"\"\n",
        "    Outputs a summary and head of the train/valid splits\n",
        "    to verify the 'White Box' state of the data.\n",
        "    \"\"\"\n",
        "    for name in split_dict.keys():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ðŸ“Š DATASET: {name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        for split_type in ['train', 'valid']:\n",
        "            df = split_dict[name][split_type]\n",
        "\n",
        "            print(f\"\\n--- {split_type.upper()} SPLIT (Shape: {df.shape}) ---\")\n",
        "\n",
        "            # Select a subset of columns for a clean view:\n",
        "            # ID, Time, Settings, a few Raw Sensors, a few Norm Sensors, and RUL\n",
        "            cols_to_show = ['unit_id', 'time_cycle', 'regime', 's_2', 's_2_norm', 's_11', 's_11_norm', 'RUL']\n",
        "\n",
        "            # Filter columns that actually exist (some sensors might be dropped)\n",
        "            existing_cols = [c for c in cols_to_show if c in df.columns]\n",
        "\n",
        "            display(df[existing_cols].head(10))\n",
        "\n",
        "# Execute audit\n",
        "audit_dataset_splits(split_data)"
      ],
      "metadata": {
        "id": "ZMxGXQh6PK59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Target Engineering: Piecewise Linear RUL\n",
        "\n",
        "In aerospace **Prognostics and Health Management (PHM)**, the relationship between time and degradation is rarely linear from the first hour of operation. Components typically experience an **Initial Stable Phase** where wear is negligible, followed by a **Degradation Phase** where failure signatures become detectable.\n",
        "\n",
        "### 3.1 The Problem with Linear RUL\n",
        "If we use a strictly linear target (e.g., $RUL = Max - Current$), we force the model to attempt to distinguish between \"Healthy at Cycle 10\" and \"Healthy at Cycle 50\". Since the sensor signatures at these points are often identical, this creates \"gradient noise\" that hinders model convergence and penalises the model for being unable to predict the unpredictable early-life state.\n",
        "\n",
        "### 3.2 The Piecewise Solution\n",
        "We implement a **Saturation Threshold** ($RUL_{limit}$). This transforms the target into a constant value during the stable phase, and a linear decline during the wear-out phase:\n",
        "\n",
        "$$RUL_{target} = \\min(RUL_{true}, RUL_{limit})$$\n",
        "\n",
        "\n",
        "\n",
        "### 3.3 Architectural Decision: $RUL_{limit} = 125$\n",
        "Following industry benchmarks for the C-MAPSS dataset (e.g., *[Heimes, 2008](https://www.researchgate.net/publication/224358896_Recurrent_neural_networks_for_remaining_useful_life_estimation)*; *[Li et al., 2018](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8723466)*), we set the threshold at **125 cycles**. This ensures:\n",
        "1. **Stable Training:** The model learns a \"Nominal\" state for all healthy engines.\n",
        "2. **Sensitive Detection:** The model focuses its learning capacity on the \"Knee\" of the curve where the degradation trajectory actually begins."
      ],
      "metadata": {
        "id": "wiyeoHEuIhjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. TARGET ENGINEERING: PIECEWISE RUL ---\n",
        "\n",
        "def apply_piecewise_label(split_dict, max_rul=125):\n",
        "    \"\"\"\n",
        "    Applies the piecewise linear RUL logic to both train and valid splits.\n",
        "    Standard threshold in literature is 125 cycles.\n",
        "    \"\"\"\n",
        "    print(f\"--- ðŸŽ¯ Applying Piecewise RUL (Threshold: {max_rul}) ---\")\n",
        "\n",
        "    for dataset_name in split_dict.keys():\n",
        "        for split_type in ['train', 'valid']:\n",
        "            df = split_dict[dataset_name][split_type]\n",
        "\n",
        "            # Create the piecewise target\n",
        "            df['RUL_piecewise'] = df['RUL'].clip(upper=max_rul)\n",
        "\n",
        "    print(\"âœ… Piecewise RUL labelling complete for all splits.\")\n",
        "\n",
        "# Execute\n",
        "apply_piecewise_label(split_data, max_rul=125)\n",
        "\n",
        "# --- VISUAL VERIFICATION ---\n",
        "# Checking a single unit from FD002 Train to confirm the \"Knee\" in the graph\n",
        "sample_unit = split_data['FD002']['train'][split_data['FD002']['train']['unit_id'] == 1]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sample_unit['time_cycle'], sample_unit['RUL'], label='Raw Linear RUL', linestyle='--', color='gray')\n",
        "plt.plot(sample_unit['time_cycle'], sample_unit['RUL_piecewise'], label='Piecewise RUL (Target)', linewidth=2, color='darkred')\n",
        "plt.axhline(y=125, color='black', linestyle=':', label='Saturation Threshold (125)')\n",
        "\n",
        "plt.title(\"Prognostic Target Architecture: Piecewise Linear RUL\", fontsize=14)\n",
        "plt.xlabel(\"Time Cycles\")\n",
        "plt.ylabel(\"Remaining Useful Life (RUL)\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "udJEYdo6Iplg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Feature Engineering: Rolling Statistics & Delta Analysis\n",
        "\n",
        "While Deep Learning architectures can inherently learn temporal patterns, providing explicitly engineered **Trend Features** significantly reduces training time and improves model robustness by reducing the search space for the optimisation algorithm.\n",
        "\n",
        "### 4.1 Feature Definitions\n",
        "* **Rolling Mean ($\\mu_{rolling}$):** Acts as a low-pass filter to remove high-frequency sensor jitter. In PHM, this represents the **Velocity** of component degradation.\n",
        "* **Rolling Standard Deviation ($\\sigma_{rolling}$):** Captures increasing signal instability. As an engine approaches EOL (End of Life), mechanical clearances and thermal efficiencies often fluctuate, leading to increased \"jitter\". This serves as a primary feature for risk-aware modelling.\n",
        "\n",
        "### 4.2 Mathematical Justification for Window Size ($W$)\n",
        "To move beyond heuristic-based window selection, we utilised **Augmented Dickey-Fuller (ADF)**, **Auto-Correlation (ACF)**, and **Partial Auto-Correlation (PACF)** functions to determine the optimal \"memory depth\" of the system.\n",
        "\n",
        "* **Stationarity (ADF Test):** Application of the ADF test to the regime-normalised signals yielded $p$-values $> 0.05$. This mathematically confirms **non-stationarity**, proving the signals contain a persistent degradation trend rather than a random walk, thus necessitating temporal models.\n",
        "* **ACF (Persistence):** The slow, linear decay in the ACF confirms long-term persistence in the signal, suggesting that the sensor state is heavily influenced by its historical trajectory.\n",
        "* **PACF (Optimal Lag):** The Partial Auto-Correlation analysis for `s_11_norm` shows significant direct correlation spikes beyond the initial lag, specifically at intervals that suggest cyclical regime-based dependencies.\n",
        "\n",
        "### 4.3 Architectural Decision: Window Size ($W = 30$)\n",
        "Based on the PACF results showing significant informational spikes at intervals up to lag 35, a window size of **30 cycles** was selected as a balanced trade-off. This provides a mathematically justified \"look-back\" that captures the core degradation memory while remaining small enough to ensure:\n",
        "1. **Responsiveness:** It captures the rapid exponential transitions in the \"wear-out\" phase.\n",
        "2. **Stability:** It provides a reliable statistical baseline for rolling calculations, effectively smoothing the high-frequency jitter observed in the raw normalised telemetry.\n",
        "\n",
        "### 4.4 Empirical Findings & Visual Audit\n",
        "Our visual audit of the engineered features (conducted on sensor `s_11_norm` for Unit 1, FD002) confirms the validity of these settings:\n",
        "\n",
        "* **Exponential Degradation Trajectory:** The Rolling Mean effectively isolates a distinct exponential curve pattern. We observe a prolonged \"Stable Phase\" followed by an accelerated \"Wear-out Phase\" starting around cycle 100.\n",
        "* **Volatility Observations:** In this specific snapshot of `s_11`, the volatility band (Rolling Std) remains relatively constant in width. The explicit calculation of variance ensures the model can distinguish between high-frequency stochastic noise and low-frequency structural drifts.\n",
        "* **Generalisation Caveat:** It is assumed that the \"widening signature\" (increasing jitter) may manifest in other sensors or across different fault modes in FD004. This investigative exercise serves as a validation of our **Trend Extraction** logic rather than a global definition of all sensor behaviours."
      ],
      "metadata": {
        "id": "vuIykvkGQOfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4.1 MATHEMATICAL AUDIT: ADF, ACF, & PACF (REVISED) ---\n",
        "\n",
        "def run_memory_audit(split_dict, dataset='FD002', unit_id=1, sensor='s_11_norm'):\n",
        "    \"\"\"\n",
        "    Empirical identification of 'Memory Depth' and Stationarity.\n",
        "    Used to justify Window Size (W).\n",
        "    \"\"\"\n",
        "    # Isolate the normalised signal for a single unit\n",
        "    series = split_dict[dataset]['train'][split_dict[dataset]['train']['unit_id'] == unit_id][sensor]\n",
        "    series = series.dropna()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸ”¬ MATHEMATICAL DEEP-DIVE: {sensor} (Unit {unit_id})\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. Augmented Dickey-Fuller (ADF) Test\n",
        "    adf_res = adfuller(series)\n",
        "    print(f\"ADF Statistic: {adf_res[0]:.4f}\")\n",
        "    print(f\"p-value: {adf_res[1]:.4f}\")\n",
        "\n",
        "    if adf_res[1] > 0.05:\n",
        "        print(\"ðŸ“ˆ RESULT: p > 0.05. Non-Stationary. Consistent degradation trend confirmed.\")\n",
        "    else:\n",
        "        print(\"ðŸ“‰ RESULT: p <= 0.05. Stationary. Signal is mean-reverting.\")\n",
        "\n",
        "    # 2. Visualising Memory Depth\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    # ACF: Overall persistence\n",
        "    plot_acf(series, lags=50, ax=ax1, title=f\"ACF: Signal Persistence ({sensor})\")\n",
        "\n",
        "    # PACF: Direct correlation (using 'yw' for compatibility)\n",
        "    plot_pacf(series, lags=50, ax=ax2, title=f\"PACF: Optimal Memory Lag ({sensor})\", method='yw')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute Audit\n",
        "run_memory_audit(split_data)"
      ],
      "metadata": {
        "id": "d9iWzRkko0I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4.2 FEATURE ENGINEERING: ROLLING STATISTICS ---\n",
        "\n",
        "def add_rolling_features(split_dict, window_size=30):\n",
        "    \"\"\"\n",
        "    Calculates rolling mean and std for all normalised sensor features.\n",
        "    Processes each unit_id independently to prevent cross-engine contamination.\n",
        "    \"\"\"\n",
        "    print(f\"--- ðŸ“ˆ Calculating Rolling Statistics (Window: {window_size}) ---\")\n",
        "\n",
        "    for dataset_name in split_dict.keys():\n",
        "        for split_type in ['train', 'valid']:\n",
        "            df = split_dict[dataset_name][split_type]\n",
        "\n",
        "            # Identify columns to roll (only the normalised sensors)\n",
        "            norm_sensors = [c for c in df.columns if '_norm' in c]\n",
        "\n",
        "            # Group by unit_id so we don't roll across different engines\n",
        "            grouped = df.groupby('unit_id')\n",
        "\n",
        "            for sensor in norm_sensors:\n",
        "                # Rolling Mean\n",
        "                df[f'{sensor}_mean'] = grouped[sensor].transform(\n",
        "                    lambda x: x.rolling(window=window_size, min_periods=1).mean()\n",
        "                )\n",
        "                # Rolling Std\n",
        "                df[f'{sensor}_std'] = grouped[sensor].transform(\n",
        "                    lambda x: x.rolling(window=window_size, min_periods=1).std()\n",
        "                ).fillna(0) # First few rows will be NaN\n",
        "\n",
        "    print(\"âœ… Rolling features added to all datasets.\")\n",
        "\n",
        "# Execute\n",
        "add_rolling_features(split_data, window_size=30)\n",
        "\n",
        "# Quick check on the new columns\n",
        "print(f\"Total features now: {len(split_data['FD002']['train'].columns)}\")\n",
        "display(split_data['FD002']['train'].filter(like='s_12').head(5))"
      ],
      "metadata": {
        "id": "FV1ubQ7-Q_qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4.3 VISUAL VERIFICATION: ROLLING VS STATIC ---\n",
        "\n",
        "# Pick a sensitive sensor (e.g., s_11: LP Spool Speed) for Unit 1 in FD002\n",
        "unit_data = split_data['FD002']['train'][split_data['FD002']['train']['unit_id'] == 1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# 1. Static Normalised Signal\n",
        "plt.plot(unit_data['time_cycle'], unit_data['s_11_norm'],\n",
        "         alpha=0.3, label='Normalised s_11 (Static)', color='gray')\n",
        "\n",
        "# 2. Rolling Mean (The Trend)\n",
        "plt.plot(unit_data['time_cycle'], unit_data['s_11_norm_mean'],\n",
        "         label='Rolling Mean (W=30)', color='blue', linewidth=2)\n",
        "\n",
        "# 3. Confidence Interval (Using Rolling Std)\n",
        "plt.fill_between(unit_data['time_cycle'],\n",
        "                 unit_data['s_11_norm_mean'] - unit_data['s_11_norm_std'],\n",
        "                 unit_data['s_11_norm_mean'] + unit_data['s_11_norm_std'],\n",
        "                 color='blue', alpha=0.1, label='Rolling Volatility (Â±1 Ïƒ)')\n",
        "\n",
        "plt.title(\"Feature Engineering: Trend Extraction (s_11_norm)\", fontsize=14)\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"Standardised Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hwlkfaYARr7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) Temporal Sequencing & Tensor Topology\n",
        "\n",
        "Modern Deep Learning architectures for time-series, such as **Long Short-Term Memory (LSTM)** networks and **Transformers**, do not process individual rows in isolation. Instead, they require **Temporal Windows** to understand the context of change.\n",
        "\n",
        "### 5.1 The 3D Tensor Transformation\n",
        "We have transformed our tabular data into a 3D coordinate system: $(Samples, Window\\_Size, Features)$.\n",
        "* **Samples:** The total number of valid sliding windows.\n",
        "* **Window Size ($W=30$):** A fixed \"look-back\" period. Each sample contains the current state plus the history of the previous 29 cycles.\n",
        "* **Features:** The high-dimensional vector containing the regime-normalised sensors and their rolling counterparts.\n",
        "\n",
        "### 5.2 Decoupled Topology Audit\n",
        "To verify the integrity of the input data, we visualised a single 3D \"slice\" (Window 0). By decoupling the features into three distinct tiers, we can audit what the model \"perceives\":\n",
        "\n",
        "1.  **Tier 1 (Static Signals):** Represents the regime-corrected state. While the manifold is collapsed, high-frequency noise is still present.\n",
        "2.  **Tier 2 (Mean Trajectories):** This is the primary driver for RUL estimation. It presents the model with clear, smoothed gradients, making the \"Exponential Wear-out\" phase mathematically prominent.\n",
        "3.  **Tier 3 (Stability Indicators):** Provides a measure of signal variance. Even if the mean is stable, an increase in Tier 3 intensity signals the onset of mechanical or thermal instability, which is a critical precursor to system failure.\n",
        "\n",
        "### 5.3 Boundary Constraint: Unit Isolation\n",
        "A critical architectural requirement of our `SequenceGenerator` is the strict enforcement of **Unit Boundaries**. The generator is programmed to reset at the start of each `unit_id`. This prevents \"cross-contamination\" where the end of one engine's life would be mathematically linked to the beginning of another'sâ€”a common error that leads to artificial performance inflation during training."
      ],
      "metadata": {
        "id": "rktGp4MCWiC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5.1 TEMPORAL SEQUENCING: 3D WINDOW GENERATOR ---\n",
        "\n",
        "def gen_sequence(df, seq_length, seq_cols):\n",
        "    \"\"\"\n",
        "    Slices a dataframe into 3D sequences of shape (Samples, seq_length, features).\n",
        "    \"\"\"\n",
        "    data_array = df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "\n",
        "    # Iterate over the range of the data to create windows\n",
        "    for start, stop in zip(range(0, num_elements - seq_length + 1), range(seq_length, num_elements + 1)):\n",
        "        yield data_array[start:stop, :]\n",
        "\n",
        "def create_tensors(split_dict, seq_length=30):\n",
        "    \"\"\"\n",
        "    Organises sequences and targets into arrays for model input.\n",
        "    \"\"\"\n",
        "    print(f\"--- ðŸ§± Generating 3D Tensors (Window Size: {seq_length}) ---\")\n",
        "\n",
        "    # Define features: Normalized sensors + Rolling Mean + Rolling Std\n",
        "    # We exclude raw sensors and settings to keep the manifold clean\n",
        "    feature_cols = [c for c in split_dict['FD002']['train'].columns if '_norm' in c]\n",
        "\n",
        "    processed_data = {}\n",
        "\n",
        "    for name in split_dict.keys():\n",
        "        processed_data[name] = {}\n",
        "        for split in ['train', 'valid']:\n",
        "            df = split_dict[name][split]\n",
        "\n",
        "            # 1. Generate Input Sequences (X)\n",
        "            # We apply the generator per unit_id and concatenate\n",
        "            X = []\n",
        "            for unit_id in df['unit_id'].unique():\n",
        "                unit_df = df[df['unit_id'] == unit_id]\n",
        "                unit_seqs = list(gen_sequence(unit_df, seq_length, feature_cols))\n",
        "                X.extend(unit_seqs)\n",
        "\n",
        "            # 2. Generate Corresponding Targets (y)\n",
        "            # For RUL, we take the value at the END of each window\n",
        "            y = []\n",
        "            for unit_id in df['unit_id'].unique():\n",
        "                unit_df = df[df['unit_id'] == unit_id]\n",
        "                # We skip the first (seq_length - 1) rows because they don't have a full window\n",
        "                unit_y = unit_df.iloc[seq_length-1:]['RUL_piecewise'].values\n",
        "                y.extend(unit_y)\n",
        "\n",
        "            processed_data[name][split] = {\n",
        "                'X': np.array(X),\n",
        "                'y': np.array(y).reshape(-1, 1)\n",
        "            }\n",
        "\n",
        "            print(f\"   âœ… {name} {split}: X={processed_data[name][split]['X'].shape}, y={processed_data[name][split]['y'].shape}\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Execute the tensor creation\n",
        "final_tensors = create_tensors(split_data, seq_length=30)"
      ],
      "metadata": {
        "id": "Rl8oF3rMWn5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5.2 DATA EXPORT ---\n",
        "\n",
        "# Define the filename for our research-ready tensors\n",
        "export_filename = 'FusionCore_3D_Tensors.pkl'\n",
        "export_path = os.path.join(DATA_PATH, export_filename)\n",
        "\n",
        "try:\n",
        "    with open(export_path, 'wb') as f:\n",
        "        pickle.dump(final_tensors, f)\n",
        "    print(f\"âœ… SUCCESS: 3D Tensors exported to {export_path}\")\n",
        "    print(f\"ðŸ“¦ Total Datasets Packaged: {list(final_tensors.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Export Failed: {e}\")"
      ],
      "metadata": {
        "id": "WVBkqMr-YrCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5.3 DECOUPLED TENSOR VISUALISATION ---\n",
        "\n",
        "def plot_decoupled_window(tensors, split_dict, dataset='FD002', split='train', window_idx=0):\n",
        "    \"\"\"\n",
        "    Visualises a single 3D window by decoupling feature types into separate subplots.\n",
        "    This prevents crowding and allows for individual trajectory audit.\n",
        "    \"\"\"\n",
        "    # 1. Extract the window and feature names\n",
        "    window_data = tensors[dataset][split]['X'][window_idx]\n",
        "    all_features = [c for c in split_dict[dataset][split].columns if '_norm' in c]\n",
        "\n",
        "    # 2. Categorise features for faceted plotting\n",
        "    norm_only = [f for f in all_features if '_mean' not in f and '_std' not in f]\n",
        "    mean_only = [f for f in all_features if '_mean' in f]\n",
        "    std_only  = [f for f in all_features if '_std' in f]\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharex=True)\n",
        "\n",
        "    # Helper to plot a group\n",
        "    def plot_group(ax, data, feature_list, title, cmap):\n",
        "        # Find indices of these features in the main window_data\n",
        "        indices = [all_features.index(f) for f in feature_list]\n",
        "        group_data = data[:, indices]\n",
        "\n",
        "        sns.heatmap(group_data.T, ax=ax, xticklabels=range(30),\n",
        "                    yticklabels=[f.replace('_norm', '') for f in feature_list],\n",
        "                    cmap=cmap, center=0, cbar_kws={'label': 'Magnitude'})\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel(\"Sensor ID\")\n",
        "\n",
        "    # 3. Generate the three tiers of the \"Temporal Image\"\n",
        "    plot_group(axes[0], window_data, norm_only, \"Tier 1: Static Regime-Normalised Signals (Raw Jitter)\", \"RdBu_r\")\n",
        "    plot_group(axes[1], window_data, mean_only, \"Tier 2: Rolling Mean Trajectories (Degradation Velocity)\", \"viridis\")\n",
        "    plot_group(axes[2], window_data, std_only,  \"Tier 3: Rolling Standard Deviation (System Instability)\", \"magma\")\n",
        "\n",
        "    plt.xlabel(\"Temporal Steps within Window (T-29 to T-0)\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Execute the audit for the first training window\n",
        "plot_decoupled_window(final_tensors, split_data, window_idx=0)"
      ],
      "metadata": {
        "id": "fE_XBdM3ZEZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL EXPORT FOR NOTEBOOK 04 ---\n",
        "export_path = os.path.join(DATA_PATH, 'FusionCore_Final_Tensors.pkl')\n",
        "\n",
        "with open(export_path, 'wb') as f:\n",
        "    pickle.dump(final_tensors, f)\n",
        "\n",
        "print(f\"âœ… FINAL SUCCESS: 3D Tensors saved to {export_path}\")\n",
        "print(\"ðŸš€ Ready to initialise Notebook 04: Model Training & Benchmarking\")"
      ],
      "metadata": {
        "id": "-tlemDkpb5yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (6) Executive Summary: Feature Engineering & Sequencing\n",
        "\n",
        "This notebook concludes the transformation of raw C-MAPSS telemetry into a research-grade 3D tensor format. By implementing a strict **Split-then-Scale** architecture, we have ensured a biologically and statistically isolated pipeline for model benchmarking.\n",
        "\n",
        "### 6.1 Key Technical Milestones\n",
        "* **Target Engineering:** Implemented a Piecewise Linear RUL with a saturation threshold of **125 cycles**. This follows the methodology established by **Heimes (2008)** and modern benchmarks by **Li et al. (2018)** to stabilise model training by ignoring early-life stochastic noise.\n",
        "* **Leakage-Proof Scaling:** Developed a protocol that learns regime centroids and sensor statistics solely from the Training split, projecting these parameters onto the Validation set to ensure zero data leakage.\n",
        "* **Trend Extraction:** Applied a 30-cycle rolling window to extract the **exponential degradation trajectory**. Visualisation of sensor `s_11_norm` confirms that the rolling mean effectively isolates the wear-out signal.\n",
        "* **3D Tensorisation:** Converted long-format data into 3D arrays of shape `(Samples, 30, Features)`. This satisfies the input requirements for Recurrent Neural Networks (RNNs) and Attention-based architectures.\n",
        "\n",
        "### 6.2 Empirical Observations\n",
        "* **Trajectory:** A distinct exponential \"acceleration\" is visible in the trend features after cycle 100.\n",
        "* **Volatility:** While volatility remained relatively constant for `s_11`, the inclusion of rolling standard deviation provides the model with the capacity to detect instability signatures in other sensors/datasets.\n",
        "\n",
        "### ðŸ“š Citations\n",
        "1. **Heimes, F. O. (2008).** *Predicting Remaining Useful Life with Recurrent Neural Networks.* 2008 International Conference on Prognostics and Health Management.\n",
        "2. **Li, X., Ding, Q., & Sun, J. Q. (2018).** *A Directed Acyclic Graph Network for Remaining Useful Life Estimation.* Applied Sciences."
      ],
      "metadata": {
        "id": "mnXwgmCWXpZB"
      }
    }
  ]
}