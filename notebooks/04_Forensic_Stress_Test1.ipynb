{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "H100",
      "authorship_tag": "ABX9TyPhhsqjynft7TT0vudS7NKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michele-Maestrini/FusionCore/blob/main/notebooks/04_Forensic_Stress_Test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FusionCore v0 â€” Prognostic Architecture & Benchmarking\n",
        "\n",
        "**Status:** `Architecture Definition`\n",
        "\n",
        "**Author:** Michele Maestrini\n",
        "\n",
        "**Previous Context:** [Feature Engineering]"
      ],
      "metadata": {
        "id": "BqNHBOwCfFzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Executive Objective: Forensic Audit & Structural Validation\n",
        "\n",
        "The primary objective of this notebook is to conduct a **Forensic Stress Test** of the predictive framework to ensure that Remaining Useful Life (RUL) estimations are derived from physical sensor signals rather than metadata artefacts. Following the discovery of near-perfect predictive accuracy in the initial baseline, this notebook serves as a **Methodological Gate** to identify, isolate, and remediate **Structural Temporal Leakage**.\n",
        "\n",
        "\n",
        "\n",
        "### 1.1 The Forensic Litmus Test\n",
        "To ensure the model is fit for safety-critical aerospace deployment, we move beyond standard accuracy metrics and subject the architecture to three validation pillars:\n",
        "\n",
        "* **Temporal Blindness Audit:** Systematically removing timestamp and index metadata to determine if the model is \"cheating\" by interpolating a linear countdown rather than interpreting thermodynamic degradation.\n",
        "* **Physics-Aware Feature Engineering:** Augmenting the dataset with kinematic derivativesâ€”**Delta ($\\Delta X$)** and **Rolling Means ($\\bar{X}$)**â€”to force the model to focus on the *velocity* of degradation (Condition-Based Monitoring) rather than simple asset age.\n",
        "* **Anomaly Sensitivity Stress Test:** Injecting synthetic sensor failures into \"healthy\" early-life assets to verify if the model prioritises physical evidence over the elapsed \"clock\". A model that ignores a catastrophic sensor spike in favour of the asset's age is deemed a safety failure.\n",
        "\n",
        "### 1.2 Success Criteria & Domain Metrics\n",
        "* **Honest Baseline Establishment:** Transitioning from an artificial $10^{-9}$ MAE to a physically grounded **MAE (~2.28)** that accurately reflects sensor noise and stochastic wear.\n",
        "* **Prognostic Logic Verification:** Success is defined by the model's ability to reduce RUL predictions in direct response to physical sensor spikes, demonstrating a \"Prognostic Reasoning\" capability.\n",
        "* **Data Readiness (v1_physics):** The final output is a sanitised, high-fidelity `.parquet` dataset, stripped of all temporal shortcuts and ready for a rigorous comparison against State-of-the-Art (SOTA) architectures in the subsequent phase."
      ],
      "metadata": {
        "id": "XuLYc37VfsGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Installation"
      ],
      "metadata": {
        "id": "0DsFlIUnuHyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoGluon for the ensemble baseline\n",
        "# PyTorch Forecasting for the TFT implementation\n",
        "!pip install -q autogluon.timeseries pytorch-forecasting"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SJHhq4FEuNKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "ivWydRpAt9y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_absolute_error"
      ],
      "metadata": {
        "id": "TZSzMxGNuAz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Environment Setup & Forensic Audit Framework\n",
        "\n",
        "To execute a rigorous stress test of our prognostic baseline, we must establish a stable computation environment capable of handling high-dimensional telemetry and synthetic anomaly injection. This section ensures the \"Digital Thread\" between the raw C-MAPSS data and our forensic validation remains unbroken.\n",
        "\n",
        "### 2.1 Dependency Management\n",
        "We utilise a specialised suite of libraries designed for statistical validation and automated benchmarking:\n",
        "* **AutoGluon (Forensic Baseline):** Used specifically as a \"black-box\" diagnostic tool to identify structural leakage and establish an initial performance ceiling.\n",
        "* **Scikit-Learn:** For implementing the shuffled tabular regression and calculating the \"Honest\" MAE baseline.\n",
        "* **Matplotlib & Seaborn:** To generate the residual plots and prognostic visualisations required to identify statistical heteroscedasticity.\n",
        "\n",
        "### 2.2 Data Recovery & Path Synchronisation\n",
        "The processed sensor data and engineered kinematics are retrieved from the Project Path (`/content/drive/MyDrive/PI/Datasets`). This section synchronises our local environment with the Google Drive repository, ensuring that all subsequent audits are performed on the same regime-normalised telemetry validated in previous steps.\n",
        "\n",
        "### 2.3 Structural Integrity Audit\n",
        "Before initiating the stress tests, we perform a validation of the data schema. This ensures:\n",
        "1. **Feature Consistency:** All engineered rolling statistics and deltas are correctly mapped across the four C-MAPSS subsets.\n",
        "2. **Target Alignment:** RUL labels are correctly associated with their respective sensor snapshots, ensuring that our \"Honest\" MAE calculation is mathematically sound.\n",
        "3. **Leakage Prevention:** Verification that metadata indices (item_id, timestamp) are available for grouping but are excluded from the training tensors to prevent temporal cheating."
      ],
      "metadata": {
        "id": "87xkDCqvuXM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Pc0ENzevufQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating to match your specific Project Path\n",
        "DATA_PATH = \"/content/drive/MyDrive/PI/Datasets\"\n",
        "TENSOR_FILE = 'FusionCore_Final_Tensors.pkl'\n",
        "\n",
        "def load_final_tensors(path):\n",
        "    \"\"\"\n",
        "    Retrieves the 3D tensors and target vectors from the established project path.\n",
        "    \"\"\"\n",
        "    target_path = os.path.join(path, TENSOR_FILE)\n",
        "    if os.path.exists(target_path):\n",
        "        with open(target_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        print(f\"âœ… Tensors successfully recovered from: {target_path}\")\n",
        "        return data\n",
        "    else:\n",
        "        # Diagnostic check: list available files if the path fails\n",
        "        print(f\"âŒ Target not found. Contents of {path}:\")\n",
        "        try:\n",
        "            print(os.listdir(path))\n",
        "        except Exception as e:\n",
        "            print(f\"Directory inaccessible: {e}\")\n",
        "        raise FileNotFoundError(f\"Could not locate {TENSOR_FILE}\")\n",
        "\n",
        "# Execute recovery\n",
        "final_tensors = load_final_tensors(DATA_PATH)"
      ],
      "metadata": {
        "id": "Hz67xEinvuOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modern PyTorch API (2.0+) for precision control\n",
        "# 'high' enables TF32 on Ampere GPUs for a performance boost with minimal precision loss.\n",
        "# 'highest' uses full float32 (IEEE) for maximum numerical rigour.\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Silencing the CuDNN TF32 warning specifically\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "P1wlNGEDAG_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3sn7045cjAg"
      },
      "outputs": [],
      "source": [
        "# --- 2.1 TENSOR SCHEMA AUDIT ---\n",
        "\n",
        "def audit_tensor_shapes(tensors):\n",
        "    \"\"\"\n",
        "    Summarises the shapes of 3D input tensors (X) and target vectors (y).\n",
        "    \"\"\"\n",
        "    audit_results = []\n",
        "\n",
        "    for ds_name in tensors.keys():\n",
        "        for split in tensors[ds_name].keys():\n",
        "            X_shape = tensors[ds_name][split]['X'].shape\n",
        "            y_shape = tensors[ds_name][split]['y'].shape\n",
        "\n",
        "            audit_results.append({\n",
        "                'Dataset': ds_name,\n",
        "                'Split': split,\n",
        "                'Samples (N)': X_shape[0],\n",
        "                'Window (W)': X_shape[1],\n",
        "                'Features (F)': X_shape[2],\n",
        "                'Target Shape': y_shape\n",
        "            })\n",
        "\n",
        "    # Create a tabulated view\n",
        "    audit_df = pd.DataFrame(audit_results)\n",
        "    display(audit_df)\n",
        "\n",
        "# Execute the audit\n",
        "audit_tensor_shapes(final_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.2 FULL TENSOR SCHEMA AUDIT ---\n",
        "\n",
        "def audit_all_subsets(tensors):\n",
        "    \"\"\"\n",
        "    Summarises the dimensions for all CMAPSS sub-datasets.\n",
        "    \"\"\"\n",
        "    audit_data = []\n",
        "    for ds in sorted(tensors.keys()):\n",
        "        for split in ['train', 'test']: # Focusing on primary splits\n",
        "            if split in tensors[ds]:\n",
        "                X_shape = tensors[ds][split]['X'].shape\n",
        "                y_shape = tensors[ds][split]['y'].shape\n",
        "                audit_data.append({\n",
        "                    'Dataset': ds,\n",
        "                    'Split': split,\n",
        "                    'Samples (N)': X_shape[0],\n",
        "                    'Window (W)': X_shape[1],\n",
        "                    'Features (F)': X_shape[2],\n",
        "                    'Target': y_shape\n",
        "                })\n",
        "\n",
        "    audit_df = pd.DataFrame(audit_data)\n",
        "    print(\"ðŸ“‹ Final Handshake: Tensor Dimensions Across All Subsets\")\n",
        "    return audit_df\n",
        "\n",
        "# Display the audit\n",
        "audit_all_subsets(final_tensors)"
      ],
      "metadata": {
        "id": "LC7hDWrz6-YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) AutoGluon Baseline: Establishing the Predictive Ceiling - Stress Test\n",
        "\n",
        "To provide a rigorous benchmark for our custom deep learning architectures, we utilise **AutoGluon-TimeSeries** as our primary baseline. AutoGluon employs an automated stacking and ensembling strategyâ€”blending multiple state-of-the-art models such as DeepAR, PatchTST, and various statistical estimatorsâ€”to determine the maximum attainable accuracy for the C-MAPSS dataset.\n",
        "\n",
        "### 3.1 The \"Prognostic Stress Test\" (FD002 & FD004)\n",
        "While we will evaluate the entire suite (FD001â€“FD004), we utilise **FD002** and **FD004** as our primary \"Stress Tests\" for architectural validation. These subsets are uniquely challenging due to:\n",
        "* **Operating Complexity:** They incorporate six distinct flight regimes. A successful model must distinguish between sensor fluctuations caused by mechanical degradation and those caused by shifts in altitude, Mach number, or throttle settings.\n",
        "* **Fault Diversity:** Unlike the simpler FD001/003 subsets, these include two distinct failure modes, requiring the model to generalise across different physical breakdown mechanisms (e.g., High-Pressure Compressor vs. Fan degradation).\n",
        "\n",
        "### 3.2 Data Re-Engineering: The Long-Format Requirement\n",
        "AutoGluon-TimeSeries operates on a **Long-Format** `TimeSeriesDataFrame` rather than the 3D tensors ($N \\times W \\times F$) required by LSTMs and Transformers. To ensure a fair comparison, we must perform a precise data re-engineering step:\n",
        "1. **Inverse Transformation:** We will reconstruct the tabular temporal sequence from our windowed tensors, ensuring that the temporal order of the 30-cycle windows is preserved.\n",
        "2. **Schema Alignment:** We will map the cycle indices and Unit IDs back to a 2D structure, where each row represents a discrete cycle in an engine's life.\n",
        "3. **Target Parity:** We must ensure the **Piecewise Linear RUL** (capped at 125 cycles) is correctly associated with the final cycle of each sequence to maintain mathematical consistency across all models in our triumvirate.\n",
        "\n",
        "By establishing this automated baseline first, we define the \"Machine Benchmark\"â€”the score that our custom-tuned **TFT** and **TSMixer** architectures must aim to outperform or, at the very least, match with significantly lower computational overhead."
      ],
      "metadata": {
        "id": "aS9_a1EP8uSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1 TENSOR TO LONG-FORMAT CONVERSION ---\n",
        "\n",
        "def transform_to_autogluon_format(tensors, dataset_name='FD002', split='train'):\n",
        "    \"\"\"\n",
        "    Converts 3D tensors back to a Long-Format DataFrame for AutoGluon.\n",
        "    We extract the final cycle of each window to map to the RUL target.\n",
        "    \"\"\"\n",
        "    X = tensors[dataset_name][split]['X']\n",
        "    y = tensors[dataset_name][split]['y']\n",
        "\n",
        "    # In Notebook 03, we established the feature names.\n",
        "    # For the audit, we'll use generic indices or map back if names are stored.\n",
        "    num_features = X.shape[2]\n",
        "    feature_cols = [f\"feat_{i}\" for i in range(num_features)]\n",
        "\n",
        "    # We take the last cycle of each 30-step window\n",
        "    # This represents the 'Current State' that corresponds to the RUL label\n",
        "    X_last_cycle = X[:, -1, :]\n",
        "\n",
        "    df = pd.DataFrame(X_last_cycle, columns=feature_cols)\n",
        "    df['target_rul'] = y\n",
        "\n",
        "    # IMPORTANT: AutoGluon needs a 'timestamp' and 'item_id'.\n",
        "    # Since we lost 'unit_id' in the raw tensor, we need to ensure the order\n",
        "    # is preserved or re-map from the original split_data if available.\n",
        "    # For the baseline, we treat the sequence as a single continuous id\n",
        "    # or map back using the sample indices.\n",
        "\n",
        "    df['item_id'] = 1 # Temporary ID for the baseline run\n",
        "    df['timestamp'] = np.arange(len(df)) # Sequential cycles\n",
        "\n",
        "    return TimeSeriesDataFrame.from_data_frame(\n",
        "        df,\n",
        "        id_column=\"item_id\",\n",
        "        timestamp_column=\"timestamp\"\n",
        "    )\n",
        "\n",
        "# Prepare the data for the AutoGluon Stress Test\n",
        "train_data_ag = transform_to_autogluon_format(final_tensors, 'FD002', 'train')\n",
        "print(f\"âœ… AutoGluon DataFrame Created. Shape: {train_data_ag.shape}\")"
      ],
      "metadata": {
        "id": "GKoRYu079nJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.2 INITIATING THE BASELINE STRESS TEST ---\n",
        "\n",
        "# Define the prediction horizon (for RUL, we typically focus on the next step)\n",
        "# However, AutoGluon requires a prediction_length >= 1\n",
        "PREDICTION_LENGTH = 1\n",
        "\n",
        "predictor = TimeSeriesPredictor(\n",
        "    target=\"target_rul\",\n",
        "    prediction_length=PREDICTION_LENGTH,\n",
        "    eval_metric=\"RMSE\",\n",
        "    path=\"autogluon_models_fd002\",\n",
        "    verbosity=2\n",
        ")\n",
        "\n",
        "# Train using the high_quality preset to establish the benchmark\n",
        "# We set a time limit to manage Google Colab resources effectively\n",
        "predictor.fit(\n",
        "    train_data_ag,\n",
        "    presets=\"high_quality\",\n",
        "    time_limit=1800,  # 30-minute stress test limit\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7wRdcZAr-_oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.3 EVALUATING THE LEADERBOARD ---\n",
        "leaderboard = predictor.leaderboard(train_data_ag)\n",
        "display(leaderboard)"
      ],
      "metadata": {
        "id": "NLKB4U0b_HPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Performance Analysis: Predictive Maintenance Leaderboard\n",
        "\n",
        "## 1. The Performance Gap: Tabular vs. Deep Learning\n",
        "The **DirectTabular** and **WeightedEnsemble** models are achieving near-zero error (order of $10^{-9}$), while heavyweights like **Temporal Fusion Transformer (TFT)** and **Chronos** are trailing by orders of magnitude.\n",
        "\n",
        "* **The Critique:** In aerospace predictive maintenance, a \"perfect\" score often signals **data leakage**. If your features include variables that are mathematically derived from the target or if the test set contains \"future\" information leaked through windowing/shuffling, your model isn't learning physicsâ€”it's learning an identity function.\n",
        "* **The Gap:** Why is TFT at $-0.13$ while DirectTabular is at $-2.47 \\times 10^{-9}$? TFT is designed to handle complex non-linearities and global patterns. If a simple tabular model beats it this decisively, your underlying signal might be highly linear or the dataset size might be too small for the TFT to converge on the manifold.\n",
        "\n",
        "## 2. Chronos and Pre-trained Models\n",
        "It is notable that **Chronos2 (Large)** outperformed the **FineTuned Small** version.\n",
        "\n",
        "* **The Methodology Flaw:** The `ChronosSmallFineTuned` model has a significantly worse test score ($-1.499$) compared to its validation score ($-0.687$). This is a textbook case of **overfitting**. In a reliability context, this is dangerous; it suggests the model has \"memorised\" the training degradation curves and fails to generalise to the unique sensor noise or initial conditions of the test units.\n",
        "* **Inference Latency:** Note the `pred_time_test`. Chronos2 is roughly **38Ã— slower** than `RecursiveTabular`. For real-time health management in an edge environment (like an orbital satellite or a turbine controller), that latency penalty is a deal-breaker unless the accuracy gain is massiveâ€”which here, it isn't.\n",
        "\n",
        "## 3. The \"NaÃ¯ve\" Baselines\n",
        "**SeasonalNaive** and **AutoETS** are sitting at scores of approximately $-1.0$.\n",
        "\n",
        "* **The Interpretation:** Since these are baselines, they define the \"floor\" of acceptable performance. Any model scoring worse than $-1.0$ (like your fine-tuned Chronos or the Regressor variant) is effectively performing worse than a model that just guesses the last known seasonal value.\n",
        "\n",
        "### Comparative Analysis of Top Models\n",
        "| Model | Test Score (MSE/MAE) | Fit Time (s) | Risk Factor |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **WeightedEnsemble** | $\\approx 0$ | 0.17 | High risk of leakage/Overfit |\n",
        "| **DirectTabular** | $\\approx 0$ | 2.46 | Strongest contender for production |\n",
        "| **TFT** | $-0.138$ | 85.20 | Computationally expensive; needs tuning |\n",
        "| **Chronos2** | $-0.257$ | 1.11 | High inference latency |\n",
        "\n",
        "### Questions for the Methodology\n",
        "To push this into a \"production-ready\" aerospace context, you should address the following:\n",
        "* **Stationarity:** Did you check for unit roots or non-stationarity in the sensor data? Tabular models often struggle with raw trends compared to state-space models.\n",
        "* **The Zero-Error Phenomenon:** Can you justify a score of $10^{-9}$? In aerospace, sensor noise usually dictates a higher error floor. Verify that you haven't included the \"Time-to-Failure\" (TTF) as a feature.\n",
        "* **Prognostics Horizon:** Is the model predicting just the next timestamp (one-step-ahead) or the entire RUL curve? One-step prediction is often trivial for maintenance planning."
      ],
      "metadata": {
        "id": "fu2DRE2dE0ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Investigating the \"Near-Perfect\" Score\n",
        "\n",
        "To identify why `DirectTabular` is achieving $10^{-9}$, we must scrutinise two areas: **Windowing Logic** and **Feature Leakage**.\n",
        "\n",
        "#### 1. The Windowing Logic\n",
        "In time-series for predictive maintenance, a common error is \"shuffling before windowing.\" If you create windows and then perform a random train-test split, the model might see windows that overlap significantly in time.\n",
        "\n",
        "\n",
        "\n",
        "* **The Test:** Are you using a **Time-Series Split** (Walk-forward validation) where the test set is strictly chronologically after the training set?\n",
        "* **The Risk:** If you used a random split on overlapping windows, the model is **interpolating** between data points it has already seen in a slightly different window.\n",
        "\n",
        "#### 2. Feature Importance & Leakage\n",
        "The fact that `DirectTabular` is winning suggests the relationship is highly structural.\n",
        "\n",
        "* **The \"ID\" Problem:** If the engine/unit ID or the \"Cycle Number\" is treated as a raw numerical feature, the model might be mapping the target directly to the cycle count rather than learning degradation physics.\n",
        "* **Lag Leakage:** If your features include $Y_{t-1}$ and you are predicting $Y_t$ in a system with very high sampling frequency, the \"score\" will look perfect because the signal hasn't changed enough to challenge the model.\n",
        "\n",
        "#### 3. Definitive Answers on Methodology\n",
        "To move from suspicion to rigour, we must perform these three tests:\n",
        "1.  **The \"Null Feature\" Test:** Train using only the Time or Cycle column. The goal here is to establish a baseline of how much information is leaked simply by knowing \"where\" we are in the life of the asset. In many datasets (like C-MAPSS), degradation is almost perfectly correlated with the cycle count.\n",
        "2.  **Permutation Importance:** Identify if a single sensor is a proxy for the target. We need to see which sensors are \"carrying\" the prediction. If one sensor is a mathematical proxy for the Remaining Useful Life (RUL), the model will exploit it.\n",
        "3.  **The Chronological Gap Test:** Introduce a \"gap\" of 10-20 cycles between training and testing. If the score collapses, leakage via temporal proximity is confirmed. This is the \"stress test\" for temporal leakage. We simulate a real-world scenario where we don't have the \"immediate past\" data points during the prediction phase"
      ],
      "metadata": {
        "id": "52KmF1-GHTji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. THE NULL FEATURE TEST ---\n",
        "\n",
        "# 1. Dataset Setup (Using your confirmed names)\n",
        "train_data_ag = globals()['train_data_ag']\n",
        "# If test_data_ag isn't defined, we use a slice of train for the test demonstration\n",
        "test_data_ag = globals().get('test_data_ag', train_data_ag)\n",
        "\n",
        "target_col = 'target_rul'\n",
        "\n",
        "# 2. Construct the Null DataFrames\n",
        "# We keep only ID, Timestamp, and the RUL target\n",
        "train_null_df = train_data_ag.reset_index()[['item_id', 'timestamp', target_col]]\n",
        "test_null_df = test_data_ag.reset_index()[['item_id', 'timestamp', target_col]]\n",
        "\n",
        "train_null = TimeSeriesDataFrame.from_data_frame(\n",
        "    train_null_df, id_column='item_id', timestamp_column='timestamp'\n",
        ")\n",
        "test_null = TimeSeriesDataFrame.from_data_frame(\n",
        "    test_null_df, id_column='item_id', timestamp_column='timestamp'\n",
        ")\n",
        "\n",
        "# 3. Fit the Null Predictor\n",
        "# Updated 'eval_metric' to 'MAE' to resolve the ValueError\n",
        "predictor_null = TimeSeriesPredictor(\n",
        "    label=target_col,\n",
        "    eval_metric=\"MAE\",\n",
        "    prediction_length=1\n",
        ")\n",
        "\n",
        "predictor_null.fit(\n",
        "    train_null,\n",
        "    hyperparameters={'DirectTabular': {}},\n",
        "    time_limit=300\n",
        ")\n",
        "\n",
        "# 4. Results Comparison\n",
        "null_leaderboard = predictor_null.leaderboard(test_null)\n",
        "print(\"\\n--- NULL FEATURE TEST RESULTS ---\")\n",
        "print(null_leaderboard[['model', 'score_test', 'score_val']])\n",
        "\n",
        "original_score = -2.472684e-09\n",
        "null_score = null_leaderboard.loc[null_leaderboard['model'] == 'DirectTabular', 'score_test'].values[0]\n",
        "\n",
        "print(f\"\\nOriginal Model Score: {original_score}\")\n",
        "print(f\"Null Model Score:     {null_score}\")"
      ],
      "metadata": {
        "id": "yMpz8UqWFEWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. PERMUTATION IMPORTANCE ---\n",
        "\n",
        "# 1. Determine the number of unique items (engines/assets) in your test set\n",
        "num_items = test_data_ag.num_items\n",
        "\n",
        "# 2. Run Permutation Importance with an explicit integer subsample_size\n",
        "# We use all items (num_items) to ensure the rigour you requested\n",
        "importance = predictor.feature_importance(\n",
        "    data=test_data_ag,\n",
        "    model='DirectTabular',\n",
        "    subsample_size=num_items\n",
        ")\n",
        "\n",
        "print(\"\\n--- TEST #2: PERMUTATION IMPORTANCE RESULTS ---\")\n",
        "# Displaying the raw importance values\n",
        "print(importance.sort_values(by='importance', ascending=False))\n",
        "\n",
        "# 3. Identify if sensors are actually contributing\n",
        "sensor_importance = importance[importance.index.str.contains('feat')]['importance'].sum()\n",
        "total_importance = importance['importance'].sum()\n",
        "\n",
        "print(f\"\\nTotal Importance: {total_importance}\")\n",
        "print(f\"Total Sensor Contribution: {sensor_importance}\")\n",
        "\n",
        "if total_importance > 0:\n",
        "    sensor_ratio = (sensor_importance / total_importance) * 100\n",
        "    print(f\"Sensor Contribution Ratio: {sensor_ratio:.2f}%\")"
      ],
      "metadata": {
        "id": "Byp8vZvyNgpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. THE CHRONOLOGICAL GAP TEST ---\n",
        "\n",
        "# 1. Setup Data\n",
        "# We will take the test data and remove the last 20 observations for each item\n",
        "# to see if the model can still predict the \"distant\" future.\n",
        "gap_size = 20\n",
        "test_data_ag = globals()['test_data_ag']\n",
        "\n",
        "def create_gapped_test(df, gap):\n",
        "    # For each item_id, drop the last 'gap' steps\n",
        "    return df.groupby(level=0, group_keys=False).apply(lambda x: x.iloc[:-gap])\n",
        "\n",
        "gapped_test_data = create_gapped_test(test_data_ag, gap_size)\n",
        "\n",
        "print(f\"Original Test Steps: {len(test_data_ag)}\")\n",
        "print(f\"Gapped Test Steps:   {len(gapped_test_data)} (Gap of {gap_size} steps created)\")\n",
        "\n",
        "# 2. Evaluate the original predictor on the gapped data\n",
        "# We check if the 'DirectTabular' model can survive without the immediate 'neighbor' data\n",
        "try:\n",
        "    gapped_leaderboard = predictor.leaderboard(gapped_test_data)\n",
        "    print(\"\\n--- TEST #3: CHRONOLOGICAL GAP RESULTS ---\")\n",
        "    print(gapped_leaderboard[gapped_leaderboard['model'] == 'DirectTabular'][['model', 'score_test']])\n",
        "\n",
        "    gapped_score = gapped_leaderboard.loc[gapped_leaderboard['model'] == 'DirectTabular', 'score_test'].values[0]\n",
        "    original_score = -2.472684e-09\n",
        "\n",
        "    print(f\"\\nOriginal (No Gap) Score: {original_score}\")\n",
        "    print(f\"Gapped (20-step) Score:  {gapped_score}\")\n",
        "\n",
        "    if np.isclose(gapped_score, original_score, atol=1e-7):\n",
        "        print(\"\\nCONCLUSION: The model is a pure Linear Counter. Even with a gap, it just follows the time-slope.\")\n",
        "    else:\n",
        "        print(\"\\nCONCLUSION: Performance collapsed. The model was relying on immediate temporal proximity (Interpolation).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Evaluation failed: {e}\")"
      ],
      "metadata": {
        "id": "Ru53wU7YO_2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoGluon Model Appraisal: Critical Methodological Failure\n",
        "\n",
        "#### 1. Executive Summary: The \"Lazy Learner\" Pathology\n",
        "Comprehensive stress testing of the `DirectTabular` model (and the `WeightedEnsemble`) confirms a catastrophic methodological failure. While the leaderboard suggests near-perfect accuracy ($10^{-9}$), this score is a mathematical artifact of **Temporal Interpolation** and **Metadata Leakage**. The model is not performing Health Management; it is performing simple arithmetic on a timestamp.\n",
        "\n",
        "#### 2. Evidence from Rigorous Stress Testing\n",
        "| Test | Objective | Result | Finding |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **1. Null Feature Test** | Remove all 63 sensors; provide only the 'Clock'. | **$-2.47 \\times 10^{-9}$** | The model ignores sensors entirely; it maps time to RUL. |\n",
        "| **2. Permutation Importance** | Measure sensor contribution to prediction. | **0.0%** | Sensor features (`feat_0`â€“`feat_62`) have zero impact on the score. |\n",
        "| **3. Chronological Gap** | Force a 20-step gap between context and target. | **Score Collapse** | Accuracy dropped by 7 orders of magnitude (to $-0.0093$). |\n",
        "\n",
        "\n",
        "\n",
        "#### 3. The Senior Reliability Engineerâ€™s \"Litmus Test\"\n",
        "In an aerospace startup environment, presenting this $10^{-9}$ leaderboard to a Senior Reliability Engineer would trigger an immediate red flag. They would pose the following challenge:\n",
        "\n",
        "> **The Question:** \"What happens if a critical sensor, such as **High Pressure Turbine Temperature**, spikes dangerously, but the engine has only been running for 10 hours?\"\n",
        "\n",
        "* **The Modelâ€™s Current Answer:** \"The engine is fine; RUL is 190 cycles. 10 hours is nowhere near the failure limit.\"\n",
        "* **The Physical Reality:** The engine is experiencing a thermal runaway and is seconds from a catastrophic \"uncontained\" failure.\n",
        "\n",
        "**Conclusion:** Because the model relies on **Temporal Interpolation** (guessing the next point based on the immediate past) rather than **Degradation Physics**, it is blind to acute anomalies. It predicts safety based on the asset's age, ignoring the physical evidence of imminent breakdown.\n",
        "\n",
        "#### 4. Required Remediation\n",
        "To move toward a production-ready Prognostics and Health Management (PHM) system:\n",
        "1.  **Exclusion of Temporal Bias:** Strictly remove absolute `timestamp` and `cycle` indices from the feature set.\n",
        "2.  **Differential Feature Engineering:** Transition to training on sensor \"deltas\" and \"gradients\" to capture the *physics of failure*.\n",
        "3.  **Window Shuffling:** Randomise training windows to break the linear continuity that allowed the model to \"cheat\" via interpolation."
      ],
      "metadata": {
        "id": "H5tIMelVQFGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transitioning to Condition-Based Monitoring (CBM)\n",
        "\n",
        "To move from a \"Clock Model\" to a \"Diagnostic Model,\" we must remove the absolute temporal markers that provided the shortcut for interpolation.\n",
        "\n",
        "### Why this Change?\n",
        "By dropping the `timestamp` and `cycle` equivalents, we force AutoGluon to find patterns solely within the **Sensor Manifold** ($feat\\_0$ to $feat\\_62$). This ensures that if a turbine temperature spikes unexpectedly, the model will recognize the degradation regardless of how many hours the engine has been in service.\n",
        "\n",
        "### Expected Result\n",
        "The `score_test` will likely drop (becoming more \"negative\") because the problem has become significantly harder. However, the resulting **Permutation Importance** will finally show non-zero values for physical sensors, signifying a model that can actually be used for aerospace reliability."
      ],
      "metadata": {
        "id": "5MnalC4mRMSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONDITION-BASED MONITORING ---\n",
        "\n",
        "# 1. Prepare Clean Data\n",
        "# We drop all columns that the Null Test identified as 'clocks'\n",
        "target_col = 'target_rul'\n",
        "\n",
        "# Define the sensor features only\n",
        "sensor_cols = [c for c in train_data_ag.columns if 'feat' in c]\n",
        "\n",
        "# Create a version of the data that only contains Sensors + Target\n",
        "# We keep item_id and timestamp ONLY for the index, not as features\n",
        "train_clean = train_data_ag[sensor_cols + [target_col]]\n",
        "test_clean = test_data_ag[sensor_cols + [target_col]]\n",
        "\n",
        "print(f\"Retraining with {len(sensor_cols)} sensors and 0 temporal features.\")\n",
        "\n",
        "# 2. Re-initialize the Predictor\n",
        "# We use a new path to avoid overwriting the 'cheating' model\n",
        "predictor_physics = TimeSeriesPredictor(\n",
        "    label=target_col,\n",
        "    eval_metric=\"MAE\",\n",
        "    prediction_length=1,\n",
        "    path=\"AutogluonModels/ag-physics-only\"\n",
        ")\n",
        "\n",
        "# 3. Fit with 'DirectTabular'\n",
        "# This time it MUST find patterns in the sensors\n",
        "predictor_physics.fit(\n",
        "    train_clean,\n",
        "    hyperparameters={'DirectTabular': {}},\n",
        "    time_limit=600\n",
        ")\n",
        "\n",
        "# 4. New Leaderboard\n",
        "physics_leaderboard = predictor_physics.leaderboard(test_clean)\n",
        "print(\"\\n--- NEW LEADERBOARD (Physics Only) ---\")\n",
        "print(physics_leaderboard[['model', 'score_test', 'score_val']])\n",
        "\n",
        "# 5. New Permutation Importance\n",
        "# This should now show non-zero importance for your sensors\n",
        "new_importance = predictor_physics.feature_importance(\n",
        "    data=test_clean,\n",
        "    model='DirectTabular',\n",
        "    subsample_size=test_clean.num_items\n",
        ")\n",
        "print(\"\\n--- NEW SENSOR IMPORTANCE ---\")\n",
        "print(new_importance.sort_values(by='importance', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "VFryL18ERv5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoGluon Model Appraisal: Structural Leakage Confirmed\n",
        "\n",
        "#### 1. The Paradox of the \"Physics-Only\" Model\n",
        "The retraining on physical sensors alone produced a result identical to the previous \"Clock\" models ($10^{-9}$). However, the **Permutation Importance** remains at absolute zero for every sensor.\n",
        "\n",
        "#### 2. The Final Diagnosis: Structural Target Leakage\n",
        "The model is not learning from the columns you provide. Instead, it is exploiting the **index-target relationship**. Because `target_rul` is a perfect linear countdown ($119, 118, 117...$) and the `timestamp` is a perfect linear count up, the `DirectTabular` model's internal preprocessing is creating an identity mapping.\n",
        "\n",
        "Essentially, the model has discovered that:\n",
        "$$Target(t) = Target(t-1) - 1$$\n",
        "As long as the model can see its own previous prediction or the relative position in the time-index, it will produce a \"perfect\" result that is completely disconnected from the engine sensors.\n",
        "\n",
        "## ##3. Engineering Reality Check\n",
        "In an aerospace startup, this model is a **\"Phantom Predictor.\"** * It works perfectly on your historical CSV because the history is a clean countdown.\n",
        "* It will fail the moment it is put on a real engine where degradation is non-linear, stochastic, or interrupted by maintenance events."
      ],
      "metadata": {
        "id": "sydcJ5UqSpXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert TimeSeriesDataFrames to standard DataFrames\n",
        "# This moves 'item_id' and 'timestamp' into regular columns and removes the temporal ordering\n",
        "train_shuffled = train_data_ag.reset_index()\n",
        "test_shuffled = test_data_ag.reset_index()\n",
        "\n",
        "# 2. Drop the temporal markers to ensure zero leakage\n",
        "# We remove 'timestamp' and 'item_id' so the model MUST look at sensors\n",
        "cols_to_drop = ['item_id', 'timestamp']\n",
        "train_shuffled = train_shuffled.drop(columns=cols_to_drop)\n",
        "test_shuffled = test_shuffled.drop(columns=cols_to_drop)\n",
        "\n",
        "# 3. Force Regression Type\n",
        "# Ensuring target_rul is a float prevents AutoGluon from inferring 'multiclass'\n",
        "train_shuffled[target_col] = train_shuffled[target_col].astype(float)\n",
        "test_shuffled[target_col] = test_shuffled[target_col].astype(float)\n",
        "\n",
        "print(f\"Flattened data ready. Training on {len(train_shuffled.columns)-1} sensors.\")"
      ],
      "metadata": {
        "id": "DWlZ3799Sz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Fit the Tabular Regression Model\n",
        "# This will likely take longer than the 10^-9 model because it's actually working\n",
        "predictor_shuffled = TabularPredictor(\n",
        "    label=target_col,\n",
        "    problem_type='regression',\n",
        "    eval_metric='mae'\n",
        ").fit(\n",
        "    train_data=train_shuffled,\n",
        "    time_limit=300,\n",
        "    presets='high_quality' # Ensures we get a rigorous look at the sensors\n",
        ")\n",
        "\n",
        "# 2. Evaluate and Generate the 'Honest' Importance\n",
        "shuffled_performance = predictor_shuffled.evaluate(test_shuffled)\n",
        "shuffled_importance = predictor_shuffled.feature_importance(test_shuffled)\n",
        "\n",
        "print(\"\\n--- SHUFFLED REGRESSION RESULTS ---\")\n",
        "print(f\"Actual MAE (The Honest Score): {shuffled_performance['mean_absolute_error']}\")\n",
        "print(\"\\nTop 10 Physical Sensors:\")\n",
        "print(shuffled_importance.head(10))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_QMaPP8HTshd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoGluon Model Appraisal: The First Genuine Diagnostic\n",
        "\n",
        "#### 1. Summary of Results\n",
        "By stripping the temporal index and forcing a **Shuffled Regression**, we have achieved an MAE of **2.28**. Unlike the previous $10^{-9}$ result, this score is derived entirely from the physical state of the asset.\n",
        "\n",
        "#### 2. Top Diagnostic Drivers (Condition-Based)\n",
        "| Feature | Importance | Significance (p-value) | Role |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **feat_41** | **4.378** | $4.45 \\times 10^{-7}$ | Primary Health Indicator |\n",
        "| **feat_23** | **3.476** | $1.29 \\times 10^{-7}$ | Secondary Health Indicator |\n",
        "| **feat_49** | **2.820** | $1.18 \\times 10^{-7}$ | Supporting Metric |\n",
        "\n",
        "#### 3. Aerospace Reliability Implications\n",
        "We have passed the \"Reliability Engineer Litmus Test.\" If **feat_41** (e.g., Exhaust Gas Temperature or Vibration) spikes, this model will now react because its prediction is mathematically tied to that sensor's value, not the cycle count.\n",
        "\n",
        "* **Previous Model:** Blind to sensor spikes; followed the clock.\n",
        "* **Current Model:** Sensitive to sensor deviations; predicts based on asset condition."
      ],
      "metadata": {
        "id": "1zuP9zUFV__E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Strategy: From \"Snapshot\" to \"Physics-Aware\" Diagnostics\n",
        "\n",
        "#### 1. The Limitation of Absolute Sensor Values\n",
        "Absolute sensor readings (e.g., $feat\\_41 = 0.85$) are often ambiguous without context. A high temperature might be normal during takeoff but catastrophic during taxiing. Absolute values alone fail to capture the **kinematics of degradation**.\n",
        "\n",
        "\n",
        "#### 2. Engineering the Physics-Aware Feature Set\n",
        "We are supplementing our top sensors with temporal context to force the model to focus on **asset behavior** rather than just **asset state**.\n",
        "\n",
        "| Feature Type | Formula      | Engineering Goal |\n",
        "| :--- | :--- | :--- |\n",
        "| **Absolute** | $X_{t}$ | Provides the current operating state |\n",
        "| **Lag-1** | $X_{t-1}$ | Provides immediate historical context |\n",
        "| **Delta** | $$\\Delta X = X_{t} - X_{t-1}$$    | Captures the **rate of degradation** (Velocity) |\n",
        "| **Rolling Mean** | $\\bar{X} = \\frac{1}{k} \\sum X_{i}$ | Filters sensor jitter and captures the **trend** |\n",
        "\n",
        "\n",
        "\n",
        "#### 3. Goal: Preventing \"Interpolation Leakage\"\n",
        "By focusing on **Deltas** and **Trends** while strictly excluding absolute timestamps, we ensure the model cannot \"cheat\" using the clock. The model must now understand that a specific **slope of degradation** in $feat\\_41$ correlates to a specific reduction in Remaining Useful Life (RUL).\n",
        "\n",
        "**The Result:** A robust Condition-Based Monitoring (CBM) model that survives real-world scrutiny from domain experts."
      ],
      "metadata": {
        "id": "Reyjo83PWJjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Identify the Top Sensors from our Shuffled Regression\n",
        "top_sensors = ['feat_41', 'feat_23', 'feat_49', 'feat_27', 'feat_53']\n",
        "\n",
        "def engineer_reliability_features(df):\n",
        "    # Ensure we are working on a copy to avoid SettingWithCopy warnings\n",
        "    df = df.copy()\n",
        "\n",
        "    for sensor in top_sensors:\n",
        "        # A. Lag-1: The previous state\n",
        "        df[f'{sensor}_lag1'] = df.groupby(level=0)[sensor].shift(1)\n",
        "\n",
        "        # B. Delta: The rate of change (Current - Previous)\n",
        "        # This is the \"Velocity\" of degradation\n",
        "        df[f'{sensor}_delta'] = df[sensor] - df[f'{sensor}_lag1']\n",
        "\n",
        "        # C. Rolling Mean (Window of 5): Smoothing the signal\n",
        "        # Captures the moving trend and filters out transient sensor noise\n",
        "        df[f'{sensor}_roll5'] = df.groupby(level=0)[sensor].transform(lambda x: x.rolling(window=5).mean())\n",
        "\n",
        "    # Fill NaNs created by lagging/rolling with 0 or backfill\n",
        "    return df.fillna(0)\n",
        "\n",
        "# 2. Apply engineering to our datasets\n",
        "train_engineered = engineer_reliability_features(train_data_ag)\n",
        "test_engineered = engineer_reliability_features(test_data_ag)\n",
        "\n",
        "# 3. Ensure we drop the target from features (except in the label position)\n",
        "# And we continue to EXCLUDE the raw timestamp/item_id as features\n",
        "feature_cols = [c for c in train_engineered.columns if 'feat' in c]\n",
        "target_col = 'target_rul'\n",
        "\n",
        "print(f\"Feature engineering complete. Total features: {len(feature_cols)}\")\n",
        "print(f\"New examples: {train_engineered.filter(like='_delta').columns.tolist()}\")\n",
        "\n",
        "# 4. Final Rigorous Fit\n",
        "predictor_final = TimeSeriesPredictor(\n",
        "    label=target_col,\n",
        "    eval_metric=\"MAE\",\n",
        "    prediction_length=1,\n",
        "    path=\"AutogluonModels/ag-final-physics\"\n",
        ")\n",
        "\n",
        "predictor_final.fit(\n",
        "    train_engineered[feature_cols + [target_col]],\n",
        "    hyperparameters={'DirectTabular': {}},\n",
        "    time_limit=600\n",
        ")\n",
        "\n",
        "# 5. Final Evaluation\n",
        "final_leaderboard = predictor_final.leaderboard(test_engineered)\n",
        "print(\"\\n--- FINAL LEADERBOARD (Engineered Physics) ---\")\n",
        "print(final_leaderboard[['model', 'score_test', 'score_val']])"
      ],
      "metadata": {
        "id": "B5Iw493AXzb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preview the newly engineered features\n",
        "# We select the primary sensor and its new temporal derivatives\n",
        "inspection_cols = ['feat_41', 'feat_41_lag1', 'feat_41_delta', 'feat_41_roll5', 'target_rul']\n",
        "\n",
        "print(\"--- Engineered Feature Preview (First Asset) ---\")\n",
        "# Looking at the first 7 rows to see the Rolling Mean and Lags populate\n",
        "print(train_engineered[inspection_cols].head(7))\n",
        "\n",
        "# 2. Statistical Verification\n",
        "print(\"\\n--- Delta Statistics ---\")\n",
        "# If the mean delta is negative, the sensor is generally decreasing as RUL decreases\n",
        "print(train_engineered[['feat_41_delta', 'feat_23_delta']].describe().loc[['mean', 'std', 'min', 'max']])"
      ],
      "metadata": {
        "id": "gMpSKBkkZNQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL TRAINING EXECUTION: PHYSICS-AWARE TIME-SERIES\n",
        "\n",
        "# 1. Define the full feature set (Original Sensors + New Engineered Features)\n",
        "# We ensure we are NOT including 'timestamp' or 'item_id' as features\n",
        "feature_cols = [c for c in train_engineered.columns if 'feat' in c]\n",
        "target_col = 'target_rul'\n",
        "\n",
        "print(f\"Final Model: Training on {len(feature_cols)} total features.\")\n",
        "\n",
        "# 2. Re-initialize the Final Predictor\n",
        "# Using a clean path to store the production-ready model\n",
        "predictor_final = TimeSeriesPredictor(\n",
        "    label=target_col,\n",
        "    eval_metric=\"MAE\",\n",
        "    prediction_length=1,\n",
        "    path=\"AutogluonModels/ag-final-production\"\n",
        ")\n",
        "\n",
        "# 3. Fit the model\n",
        "# We use DirectTabular because we've engineered the 'Time' context manually\n",
        "# through Lags and Deltas, so it no longer needs to 'cheat' with the index.\n",
        "predictor_final.fit(\n",
        "    train_engineered[feature_cols + [target_col]],\n",
        "    hyperparameters={'DirectTabular': {}},\n",
        "    time_limit=900\n",
        ")\n",
        "\n",
        "# 4. Final Leaderboard Evaluation\n",
        "final_leaderboard = predictor_final.leaderboard(test_engineered)\n",
        "print(\"\\n--- FINAL PRODUCTION LEADERBOARD ---\")\n",
        "print(final_leaderboard[['model', 'score_test', 'score_val']])\n",
        "\n",
        "# 5. Final Importance Check\n",
        "# This will show if our 'Delta' and 'Roll' features are outperforming raw sensors\n",
        "final_importance = predictor_final.feature_importance(\n",
        "    data=test_engineered,\n",
        "    model='DirectTabular',\n",
        "    subsample_size=test_engineered.num_items\n",
        ")\n",
        "\n",
        "print(\"\\n--- TOP PRODUCTION FEATURES ---\")\n",
        "print(final_importance.sort_values(by='importance', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "r5sZpt8IZrt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoGluon Model Appraisal: The Time-Series \"Black Box\" Leakage\n",
        "\n",
        "#### 1. Executive Summary: The \"Lazy Learner\" Pathology\n",
        "Comprehensive stress testing of the `TimeSeriesPredictor` reveals a catastrophic methodological artifact. Despite leaderboards suggesting near-perfect accuracy ($10^{-9}$), this score is a product of **Internal Structural Leakage**. The AutoGluon TimeSeries engine is so efficient at identifying the linear countdown of the `target_rul` within the `TimeSeriesDataFrame` index that it effectively bypasses all physical sensor inputs.\n",
        "\n",
        "#### 2. The Senior Reliability Engineerâ€™s \"Litmus Test\"\n",
        "In an aerospace startup environment, presenting a $10^{-9}$ leaderboard would trigger immediate scrutiny. A domain expert would pose the following challenge:\n",
        "\n",
        "> **The Question:** \"What happens if a critical sensor, such as **High Pressure Turbine Temperature**, spikes dangerously, but the engine has only been running for 10 hours?\"\n",
        "\n",
        "* **The Time-Series Modelâ€™s Answer:** \"The engine is fine; RUL is 190 cycles. 10 hours is nowhere near the failure limit.\"\n",
        "* **The Physical Reality:** The engine is experiencing thermal runaway and is seconds from catastrophic failure.\n",
        "\n",
        "**Conclusion:** By \"counting down\" the RUL based on the internal index, the Time-Series model is blind to acute anomalies. It predicts safety based on age, ignoring physical evidence of imminent breakdown.\n",
        "\n",
        "#### 3. Evidence of Internal Leakage & Remediation\n",
        "\n",
        "\n",
        "| Approach | Score (MAE) | Sensor Importance | Validity |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Initial Time-Series** | $10^{-9}$ | 0.0 | **FAILED** (Index Leakage) |\n",
        "| **Physics-Aware TS** | $10^{-9}$ | 0.0 | **FAILED** (Internal Index Leakage) |\n",
        "| **Shuffled Tabular** | **2.28** | **High (feat_41, feat_23)** | **SUCCESS** (True Physics) |\n",
        "\n",
        "#### 4. The Verdict: The \"Tabular-Only\" Path\n",
        "The `TimeSeriesPredictor` is too \"smart\" for this dataset format. Even when temporal columns are stripped, the internal engine reconstructs the sequence. To deliver a model that captures **Condition-Based Monitoring (CBM)**, one must abandon the Time-Series object for this specific problem and utilize a **Shuffled Tabular Regression**.\n",
        "\n",
        "The Shuffled Tabular model is the only architecture \"blind\" enough to be forced to listen to the sensors, yielding an honest, physics-driven baseline of **2.28 MAE**."
      ],
      "metadata": {
        "id": "T2W_NWFJaq9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: THE ANOMALY INJECTION SCRIPT ---\n",
        "\n",
        "# 1. Identify a \"Healthy\" sample (Early in life, high RUL)\n",
        "# We'll take a row where target_rul is > 100\n",
        "healthy_sample = test_shuffled[test_shuffled[target_col] > 100].iloc[[0]].copy()\n",
        "original_rul = healthy_sample[target_col].values[0]\n",
        "\n",
        "# 2. Inject the Anomaly\n",
        "# We spike 'feat_41' (our top sensor) to a value far outside normal range\n",
        "# or simply to its maximum to simulate a component failure\n",
        "spike_value = train_shuffled['feat_41'].max() * 2\n",
        "anomalous_sample = healthy_sample.copy()\n",
        "anomalous_sample['feat_41'] = spike_value\n",
        "\n",
        "print(f\"Injection Complete.\")\n",
        "print(f\"Original feat_41: {healthy_sample['feat_41'].values[0]:.4f}\")\n",
        "print(f\"Spiked feat_41:   {anomalous_sample['feat_41'].values[0]:.4f}\")\n",
        "print(f\"Ground Truth RUL: {original_rul}\")"
      ],
      "metadata": {
        "id": "H1ps-MLGbszN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 2: THE CONFRONTATION (TIME-SERIES VS TABULAR) ---\n",
        "\n",
        "# 1. Test the 'Honest' Shuffled Tabular Model\n",
        "tab_pred = predictor_shuffled.predict(anomalous_sample)\n",
        "\n",
        "# 2. Test the 'Cheating' Time-Series Model\n",
        "# (Requires converting back to TimeSeriesDataFrame to trigger its internal logic)\n",
        "from autogluon.timeseries import TimeSeriesDataFrame\n",
        "# We create a dummy TS frame for the anomalous sample\n",
        "anom_ts = TimeSeriesDataFrame.from_data_frame(\n",
        "    test_data_ag.reset_index().iloc[[0]].copy().assign(feat_41=spike_value),\n",
        "    id_column='item_id',\n",
        "    timestamp_column='timestamp'\n",
        ")\n",
        "ts_pred = predictor.predict(anom_ts)\n",
        "\n",
        "print(\"\\n--- ANOMALY STRESS TEST RESULTS ---\")\n",
        "print(f\"Original Time-Series Prediction: {ts_pred['mean'].values[0]:.2f} cycles\")\n",
        "print(f\"Honest Tabular Prediction:      {tab_pred.values[0]:.2f} cycles\")\n",
        "\n",
        "# Logic Check\n",
        "if abs(ts_pred['mean'].values[0] - original_rul) < 2:\n",
        "    print(\"\\nVERDICT: The Time-Series model IGNORED the sensor spike. It is purely following the clock.\")\n",
        "\n",
        "if tab_pred.values[0] < original_rul:\n",
        "    print(f\"VERDICT: The Tabular model REACTED to the physics. It reduced RUL by {original_rul - tab_pred.values[0]:.2f} cycles due to the sensor spike.\")"
      ],
      "metadata": {
        "id": "elUk52tBcC3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Safety Validation: Anomaly Stress Test Results\n",
        "\n",
        "#### 1. Objective\n",
        "To evaluate model reliability when faced with a \"Premature Failure\" signal. This test determines if a model is performing **Condition-Based Monitoring (CBM)** or simply **Scheduled Accounting**.\n",
        "\n",
        "#### 2. The Stress Scenario\n",
        "* **Asset State:** Early Life (Cycle 0, Ground Truth RUL: 119.0).\n",
        "* **Anomalous Event:** A catastrophic spike injected into **feat_41** (Top Health Indicator).\n",
        "* **Sensor Delta:** The reading was forced from a baseline of **-0.2238** to a critical **4.5202**.\n",
        "\n",
        "#### 3. Comparative Response\n",
        "| Model | Prediction | Delta from Truth | Safety Verdict |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Time-Series ($10^{-9}$)** | **117.13 Cycles** | ~2 Cycles | **CRITICAL FAILURE** |\n",
        "| **Shuffled Tabular (2.28)** | **101.74 Cycles** | **17.26 Cycles** | **VALIDATED** |\n",
        "\n",
        "#### 4. Final Conclusion for Startup Portfolio\n",
        "The Time-Series model is **blind to physics**. Despite a catastrophic sensor failure, it chose to trust the internal \"Clock,\" only subtracting the elapsed time.\n",
        "\n",
        "The **Shuffled Tabular Model**, despite having a \"lower\" accuracy on paper, demonstrated genuine **prognostic reasoning**. By slashing the RUL by 17.26 cycles in response to the spike, it proved it is the only model capable of protecting high-value aerospace assets from unpredicted failures."
      ],
      "metadata": {
        "id": "rS8XLA5Uc_0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- HONEST DIAGNOSTIC VISUALISATIONS ---\n",
        "\n",
        "# 1. Generate predictions for the entire shuffled test set\n",
        "predictions = predictor_shuffled.predict(test_shuffled)\n",
        "actuals = test_shuffled[target_col]\n",
        "residuals = actuals - predictions\n",
        "\n",
        "# Create a figure with two subplots: Regression and Residuals\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# --- PLOT 1: PREDICTED VS. ACTUAL ---\n",
        "sns.scatterplot(x=actuals, y=predictions, alpha=0.3, color='#0047AB', ax=ax1, label='Model Predictions')\n",
        "line_coords = [actuals.min(), actuals.max()]\n",
        "ax1.plot(line_coords, line_coords, color='#D22B2B', linestyle='--', linewidth=2, label='Perfect Prognostic')\n",
        "\n",
        "ax1.set_title('Prognostic Performance: Predicted vs. Actual RUL', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Ground Truth RUL (Cycles)', fontsize=12)\n",
        "ax1.set_ylabel('Predicted RUL (Cycles)', fontsize=12)\n",
        "ax1.legend()\n",
        "ax1.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "# Overlay Metrics\n",
        "r2 = r2_score(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "stats_text = f'RÂ²: {r2:.4f}\\nMAE: {mae:.2f}'\n",
        "ax1.text(0.05, 0.90, stats_text, transform=ax1.transAxes, fontsize=12,\n",
        "         fontweight='bold', bbox=dict(facecolor='white', edgecolor='black', alpha=0.8))\n",
        "\n",
        "# --- PLOT 2: RESIDUAL PLOT ---\n",
        "sns.scatterplot(x=predictions, y=residuals, alpha=0.3, color='#4B0082', ax=ax2)\n",
        "ax2.axhline(0, color='#D22B2B', linestyle='--', linewidth=2)\n",
        "\n",
        "ax2.set_title('Residual Analysis: Error Distribution', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Predicted RUL (Cycles)', fontsize=12)\n",
        "ax2.set_ylabel('Error (Actual - Predicted)', fontsize=12)\n",
        "ax2.grid(True, linestyle=':', alpha=0.6)\n",
        "\n",
        "# Summary Text for Residuals\n",
        "ax2.text(0.05, 0.05, \"Points above 0: Underestimating Wear\\nPoints below 0: Overestimating Wear\",\n",
        "         transform=ax2.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YF4OV0JheOk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model Validation: Safety & Statistical Rigour\n",
        "\n",
        "#### 1. Safety Validation (Anomaly Stress Test)\n",
        "**Scenario:** Catastrophic spike in **feat_41** at Cycle 0 (Truth RUL: 119.0).\n",
        "* **Clock-Based Model:** Predicted **117.13** (Failed to see the fault).\n",
        "* **Physics-Based Model:** Predicted **101.74** (Successfully reacted to the fault).\n",
        "\n",
        "**Finding:** The Shuffled Tabular model is sensitive to physical degradation, reducing predicted RUL by **17.26 cycles** purely due to sensor deviation.\n",
        "\n",
        "#### 2. Statistical Validation (Residual Analysis)\n",
        "While the Anomaly Test proves **sensitivity**, the Residual Plot proves **consistency**.\n",
        "\n",
        "\n",
        "\n",
        "#### **Interpretation of Errors:**\n",
        "* **Zero-Line Center:** The errors are balanced around the zero-axis, confirming our model is an **Unbiased Estimator**. It does not systematically overestimate or underestimate safety.\n",
        "* **Heteroscedasticity Check:** Observe the \"spread\" of the residuals as Predicted RUL decreases.\n",
        "    * *In Aerospace:* We expect wider variance at low RUL (near failure) as component wear becomes non-linear and chaotic.\n",
        "* **Outlier Identification:** Points far from the zero-line indicate specific assets where the sensor-to-wear mapping deviatedâ€”likely due to secondary failure modes not captured by the primary sensors.\n",
        "\n",
        "#### 3. The \"PHM\" Audit Conclusion\n",
        "The combination of **Physics Sensitivity** (Test 1) and **Unbiased Residuals** (Test 2) confirms this model as a viable framework for **Prognostics and Health Management (PHM)**. It rejects the \"temporal shortcuts\" of black-box time-series models in favour of traceable, condition-based diagnostics."
      ],
      "metadata": {
        "id": "N0bH1NcEf5Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- OUTLIER IDENTIFICATION: FAILURE MODE AUDIT ---\n",
        "\n",
        "# 1. Calculate residuals and attach to the original test data\n",
        "# We use the raw test_shuffled before dropping IDs to track specific engines\n",
        "results_df = test_shuffled.copy()\n",
        "results_df['predicted_rul'] = predictions\n",
        "results_df['residual'] = residuals\n",
        "results_df['abs_residual'] = results_df['residual'].abs()\n",
        "\n",
        "# 2. Identify the 'Dangerous' Outliers\n",
        "# (Where Actual RUL is much LOWER than Predicted - Overestimating safety)\n",
        "dangerous_outliers = results_df.sort_values(by='residual', ascending=True).head(5)\n",
        "\n",
        "print(\"--- CRITICAL OUTLIERS: OVERESTIMATED SAFETY ---\")\n",
        "# If you kept item_id in a previous step, it would show up here\n",
        "print(dangerous_outliers[['feat_41', 'feat_23', 'target_rul', 'predicted_rul', 'residual']])\n",
        "\n",
        "# 3. Identify the 'Conservative' Outliers\n",
        "# (Where Actual RUL is much HIGHER than Predicted - Wasted asset life)\n",
        "conservative_outliers = results_df.sort_values(by='residual', ascending=False).head(5)\n",
        "\n",
        "print(\"\\n--- CONSERVATIVE OUTLIERS: PREMATURE REPLACEMENT ---\")\n",
        "print(conservative_outliers[['feat_41', 'feat_23', 'target_rul', 'predicted_rul', 'residual']])"
      ],
      "metadata": {
        "id": "Xk9_wsWTgVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Thesis Appraisal: Error Distribution and Outlier Risks\n",
        "\n",
        "#### 1. Statistical Validity\n",
        "The **Residual Plot** confirms that the model is **homoscedastic** across the majority of the flight envelope, meaning its predictive reliability is stable. However, the dispersion increases slightly at lower RUL values, a common phenomenon in aerospace where mechanical wear accelerates non-linearly.\n",
        "\n",
        "#### 2. Risk Categorization\n",
        "Through the residual audit, we identify two distinct types of predictive failure:\n",
        "\n",
        "##### **Category A: Overestimated Safety (Negative Residuals)**\n",
        "* **The Risk:** The model predicts 50 cycles remaining, but the asset fails in 10.\n",
        "* **Root Cause:** Likely a \"Sudden Death\" failure mode (e.g., Foreign Object Damage) that doesn't show the typical slow degradation curve the model learned.\n",
        "* **Mitigation:** Requires a higher safety margin (Confidence Interval) for lower RUL predictions.\n",
        "\n",
        "##### **Category B: Overestimated Wear (Positive Residuals)**\n",
        "* **The Risk:** The model predicts 5 cycles remaining, but the asset could have run for 40 more.\n",
        "* **Root Cause:** Highly resilient assets or \"Golden Engines\" that perform better than the population average.\n",
        "* **Economic Impact:** Leads to premature replacement and wasted operational life.\n",
        "\n",
        "#### 3. Deployment Conclusion\n",
        "By moving from a \"cheating\" $10^{-9}$ Time-Series model to this **Honest Tabular Model**, we have traded illusory perfection for **actionable uncertainty**. We can now quantify the risk of a \"Category A\" failure, providing a rigour that is required for safety-critical aerospace certification."
      ],
      "metadata": {
        "id": "l9rcW6bFgjjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set the specific Dataset Location\n",
        "DATA_PATH = \"/content/drive/MyDrive/PI/Datasets\"\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# 2. Refined Engineering Logic (British English / Physics-Centric)\n",
        "def engineer_reliability_features_v2(df):\n",
        "    \"\"\"\n",
        "    Constructs kinematic features whilst ensuring zero temporal leakage.\n",
        "    Uses backfilling to maintain physical consistency at sequence onset.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    top_sensors = ['feat_41', 'feat_23', 'feat_49', 'feat_27', 'feat_53']\n",
        "\n",
        "    for sensor in top_sensors:\n",
        "        # Grouping by item_id (level 0) to prevent cross-engine leakage\n",
        "        engine_group = df.groupby(level=0)[sensor]\n",
        "\n",
        "        # Lag-1: The immediate preceding state\n",
        "        df[f'{sensor}_lag1'] = engine_group.shift(1)\n",
        "\n",
        "        # Delta: Current - Previous (Rate of degradation / Velocity)\n",
        "        df[f'{sensor}_delta'] = df[sensor] - df[f'{sensor}_lag1']\n",
        "\n",
        "        # Rolling Mean: Window of 5 for signal-to-noise smoothing\n",
        "        df[f'{sensor}_roll5'] = engine_group.transform(lambda x: x.rolling(window=5).mean())\n",
        "\n",
        "    # Backfill NaNs at the start of each engine sequence to maintain consistency\n",
        "    return df.groupby(level=0, group_keys=False).apply(lambda x: x.bfill()).fillna(0)\n",
        "\n",
        "# 3. Execute Engineering\n",
        "train_engineered = engineer_reliability_features_v2(train_data_ag)\n",
        "test_engineered = engineer_reliability_features_v2(test_data_ag)\n",
        "\n",
        "# 4. Strip Metadata & Sanitise Index\n",
        "# We explicitly select ONLY the features and target, then drop the index\n",
        "# to prevent 'item_id' metadata from leaking into the Parquet file.\n",
        "feature_cols = [c for c in train_engineered.columns if 'feat' in c]\n",
        "target_col = 'target_rul'\n",
        "final_cols = feature_cols + [target_col]\n",
        "\n",
        "train_sanitised = train_engineered[final_cols].reset_index(drop=True)\n",
        "test_sanitised = test_engineered[final_cols].reset_index(drop=True)\n",
        "\n",
        "# 5. Export to Parquet (index=False is the final safety gate)\n",
        "train_sanitised.to_parquet(os.path.join(DATA_PATH, 'train_physics_v1.parquet'), index=False)\n",
        "test_sanitised.to_parquet(os.path.join(DATA_PATH, 'test_physics_v1.parquet'), index=False)\n",
        "\n",
        "print(f\"âœ… Final Forensic Export Complete.\")\n",
        "print(f\"Location: {DATA_PATH}\")\n",
        "print(f\"Verification: item_id index dropped to prevent metadata leakage.\")"
      ],
      "metadata": {
        "id": "eyAqHhdYipc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Conclusion: Validation Gate 04\n",
        "\n",
        "## 1. Audit Findings\n",
        "We have successfully identified that the initial near-perfect performance ($10^{-9}$ MAE) was an artefact of **Structural Temporal Leakage**. The `TimeSeriesPredictor` was utilising the internal indexing of the `TimeSeriesDataFrame` to 'look up' the RUL countdown rather than learning from sensor physics.\n",
        "\n",
        "## 2. Remediation Strategy\n",
        "* **Temporal Blindness:** We transitioned to a **Shuffled Tabular Regression** approach to break the model's reliance on the clock.\n",
        "* **Kinematic Augmentation:** We engineered **Delta** ($\\Delta X$) and **Rolling Mean** ($\\bar{X}$) features for the top-performing sensors (`feat_41`, `feat_23`, etc.) to provide the model with 'velocity' and 'trend' context.\n",
        "\n",
        "## 3. Results & Safety Validation\n",
        "* **Honest Baseline:** Established an MAE of **~2.28**, which is a realistic and physically grounded score for this dataset.\n",
        "* **Anomaly Sensitivity:** The model proved its prognostic capability by reacting to a synthetic sensor spike with a **17.26 cycle reduction** in RUL, whereas the 'cheating' model ignored the fault entirely.\n",
        "\n",
        "## 4. Data Readiness\n",
        "The physics-aware features have been locked into `train_physics_v1.parquet` and `test_physics_v1.parquet` in the Google Drive directory. These datasets are stripped of all temporal indices, ensuring that the SOTA models in Notebook 05 must compete purely on physical signal processing."
      ],
      "metadata": {
        "id": "teUERi04mQba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ‰ Bridge to Notebook 05: SOTA Benchmarking\n",
        "\n",
        "The Forensic Audit is complete. We have successfully:\n",
        "1. Identified structural temporal leakage.\n",
        "2. Remediated the dataset by engineering kinematic derivatives.\n",
        "3. Established a physically grounded baseline of **2.28 MAE**.\n",
        "4. Exported the sanitised **v1_physics** tensors to Google Drive.\n",
        "\n",
        "**Next Objective:** Benchmark the three SOTA architectures:\n",
        "* **DeepAR:** To quantify aleatoric uncertainty and risk.\n",
        "* **TFT:** To audit attention weights against thermodynamic signals.\n",
        "* **PatchTST:** To extract local semantic patterns from multivariate sequences."
      ],
      "metadata": {
        "id": "v6e_3xf0gled"
      }
    }
  ]
}